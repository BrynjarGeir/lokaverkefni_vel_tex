% Chapter Template

\chapter{Model architecture} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Model structure}
The structure of the neural network is such that it contains some number $n$ of fully connected layers and batch normalization for each layer, along with regularization. All of these layers have the same number of units. The last layer has a dropout of 50\%. In addition to these layers there is one more output layer. This is simply a dense layer with 1 unit. A grid search was performed to determine the hyperparameters that minimize the loss. Hyperparameters are parameters that are set before the training begins\cite{hyperparameters_definition}. These hyperparameters include number of units in each layer, number of epochs to train for, number of layers, batch size, optimizer and penalty to enforce in the regularization. The possible combinations tried can be seen in Table \ref{table:gridSearchHyperparamters}.

\begin{table}[h]
    \centering
    \caption[Hyperparamter search with best performing combination.]{Hyperparamter search with best performing combination shown. Hyperparameter search was done using hyperband algorithm that initially searches randomly for the best parameters but then hones in on what is working and as such is neither exhaustive nor completely random. This means that an upper limit will not be set on the number of combinations to try like with randomsearch.}
    \label{table:gridSearchHyperparamters}
    \begin{tabular}{c|c|c}
        Parameter & Range of values & Selected\\\hline
        Layers &  min\_value = 4, max\_value = 15, step = 1 & \textbf{10}\\\hline
        Units &  min\_value = 32, max\_value = 512, step = 32 & \textbf{64}\\\hline
        Penalties & min\_value = 1e-5, max\_value = 1, sampling = log & \textbf{1e-4}\\\hline
        Epochs & min\_value = 10, max\_value = 1000, step = 10 & \textbf{30}\\\hline
        Optimizers & Adam, RMSprop, Adamax & \textbf{Adamax}\\\hline
        Activation & ReLU, ELu, Softmax & \textbf{ReLU}\\\hline
    \end{tabular}
\end{table}

\section{Feature selection and importance}
Using the three datasources, a model is trained. The data from IMO only represents the ground truth and is not used as part of the training data. The gust ($f_g$) and wind speed ($f$) are used to calculate the target gust factor. All weather parameters used in predictions come from the CARRA reanalysis data, with variables wind speed, wind direction, temperature, pressure. All of these variables are queried in three height levels (15, 250, 500 meters above ground) at the location of a given weather station. The second data source for the training data, is the elevation data. Using the GeoTIFF file that IMO provided, a sector upwind were looked at.

\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.6]{Figures/shap_bar_nn_256_example100.png}
    \caption[Feature importance of a neural network.]{Feature importance of a neural network with model architecture as described in table \ref{table:gridSearchHyperparamters} and data as described in table \ref{table:trainDataExample}.}
    \label{fig:ShapleyWaterfallFirstTry}
\end{figure}