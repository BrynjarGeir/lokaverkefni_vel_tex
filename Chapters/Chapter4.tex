% Chapter Template

\chapter{Model architecture} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Model structure}
The structure of the neural network is such that it contains some number n of  fully connected layers and batch normalization for each layer, along with regularization. All of these layers have the same number of units. The last layer has a dropout of 50\%. In addition to these layers there is one more output layer. This is simply a dense layer with 1 unit. A grid search was performed to figure out the best hyper parameters. These hyperparameters include number of units in each layer, number of epochs to train for, number of layers, batch size, optimizer and penalty to enforce in the regularization. The possible combinations tried can be seen in table \ref{table:gridSearchHyperparamters}.

\begin{table}[h]
    \centering
    \begin{tabular}{c|c}
        Layers &  4, \textcolor{red}{5}, 6, 7, 8, 9\\\hline
        Units &  128, 256, 512\\\hline
        Epochs & 100, 200, 500\\\hline
        Batch size & 32, 64, 128, 256\\\hline
        Optimizers & Adam, RMSprop, Adamax\\\hline
        Penalties & 1, 0.1, 0.01, 0\\\hline
    \end{tabular}
    \caption{Hyperparamter search with best performing combination colored red}
    \label{table:gridSearchHyperparamters}
\end{table}

\section{Feature selection and importance}
Using the three datasources, we train a model. The data from Veðurstofa only represents the ground truth and is not used as part of the training data. We use the gust ($f_g$) and wind speed ($f$) to calculate the target gust factor. From the Carra reanalysis data we get all of our weather information, with variables wind speed, wind direction, temperature, pressure. All of these variables are queried in three height levels (15, 250, 500 meters above ground) at the location of a given weather station. In addition we have the elevation above sea level over Iceland. We select the elevation in a couple of different shapes. A sector upwind, a sector upwind and downwind as well as a circle surrounding a weather station.

To add to these observed values and reanalysis, we calculate two derived variables, the Richardson number and the Brunt-Väisälä frequency. To begin with we simply use all the features we have to train our model along with the derived variables to see. We do this to be able to then use tools such as Shapley to see which features are impacting our predictions. Using Shapley values to see which features, will then allow us to exclude features that don't affect our model prediction and simplify the training data. We thus start with much higher dimensionality of our training data and reduce it as much as we can without impacting our results. This crystallizes in the landscape points. We start with n points that describe the elevation of the landscape around our weather station. This n might be in the hundreds. If we look at concentric circles outward from the weather station, we might have 10 circles (largest with radius 20 km), each with 72 points (5°spacing) and end up with 720 points for the landscape. This might slow down training. In addition, we might imagine that we are more interested in certain important landscape features. There might be a large mountain up- or downwind of our weather station, that might be the most important feature, in addition to some hills and such. To address this we use principal component analysis (PCA) to try to reduce the dimensionality of our landscape data. Another thing that might serve the same purpose is to select some number of m points that have the most increase or decrease in elevation from the weather station along with the coordinates of those points in relation to the weather station and the direction of the wind.

Using all the variables from Carra, along with the two derived factors (Ri, N) and two sectors (upwind and downwind) downscaled to 10 features using PCA we can see the feature importance in figure \ref{fig:ShapleyWaterfallFirstTry}. Looking at the waterfall graph we can see that Richardson number for higher levels is most important.

\begin{figure}[h]
    \centering
    \includegraphics[scale = 0.6]{Figures/shap_bar_nn_256_example100.png}
    \caption{Feature importance of a neural network as described with 5 hidden layers and 256 units in each}
    \label{fig:ShapleyWaterfallFirstTry}
\end{figure}